{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2428d8",
   "metadata": {},
   "source": [
    "# RL25 Exam Template\n",
    "This task is designed to evaluate your understanding of RL principles, your ability to implement algorithms, and your skill in analyzing and presenting results.\n",
    "Please **read the instructions carefully** and make sure to follow each step in your submission.\n",
    "## ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c674b",
   "metadata": {},
   "source": [
    "## Please fill the notebook with your code. This notebook should be clean, well-documented,\n",
    "## and executable from start to finish without errors.\n",
    "## ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b41e8",
   "metadata": {},
   "source": [
    "### Student Name:\n",
    "### Student ID:\n",
    "### Date:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4199bc",
   "metadata": {},
   "source": [
    "## Tips & reminders\n",
    "\n",
    "- Choose an algorithm appropriate for the task.\n",
    "- Focus on clear, readable code and meaningful comments.\n",
    "- If needed, use environment wrappers, logging, and seeding wisely.\n",
    "- Save and test your agent after training to ensure reproducibility.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c58639",
   "metadata": {},
   "source": [
    "# Environment: inverted pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0df8e",
   "metadata": {},
   "source": [
    "The environment we will be dealing with in this task is the inverted pendulum from (https://gymnasium.farama.org/), shown in the GIF below.\n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*TNo3x9zDi1lVOH_3ncG7Aw.gif)\n",
    "\n",
    "To implement this environment, you should make use of the gymnasium library. Please install the gymnasium library within your preferred Python environment using:\n",
    "\n",
    "```pip install gymnasium```\n",
    "\n",
    "Note that the episodes in this environment end with a truncation and not a termination. Assume that for the environment truncation and termination is the same. That is wherever, we are looking for a terminal state we instead look for the state where the episode is truncated! Generally, termination and truncation are different things. This is the case because the timelimit is not actually part of the MDP but rather a constraint set from the outside. More on this [here](https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2a6bf",
   "metadata": {},
   "source": [
    "Import the packages required for your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25ad4c",
   "metadata": {},
   "source": [
    "Check if the installation and import work by simulating the environment with randomly sampled actions. A window with an animation of the pendulum should open, display some random actions, and close automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ec538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb278b",
   "metadata": {},
   "source": [
    "The inverted pendulum comes with continuous action and state spaces.\n",
    "The pendulum has three state variables relating to the momentary angular position $\\theta$ and the angular velocity $\\frac{\\text{d}}{\\text{d}t}\\theta$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    x=\\begin{bmatrix}\n",
    "    \\text{cos}(\\theta)\\\\\n",
    "    \\text{sin}(\\theta)\\\\\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\theta\n",
    "    \\end{bmatrix}\n",
    "    \\in\n",
    "    \\begin{bmatrix}\n",
    "    [-1, 1]\\\\\n",
    "    [-1, 1]\\\\\n",
    "    [-8 \\, \\frac{1}{\\text{s}}, 8 \\, \\frac{1}{\\text{s}}]\n",
    "    \\end{bmatrix},\n",
    "\\end{align*}\n",
    "$$\n",
    "and one input variable which relates to the torque applied at the axis of rotation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    u = T \\in [-2 \\, \\text{N}\\cdot\\text{m}, 2 \\, \\text{N}\\cdot\\text{m}].\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38aa4a",
   "metadata": {},
   "source": [
    "The goal of this environment is to bring the pendulum into the upper neutral position, where the angle $\\theta = 0$ and the angular velocitiy $\\frac{\\text{d}}{\\text{d}t}\\theta=\\omega=0$. The environment's reward function is already designed to represent this objective:\n",
    "\n",
    "\\begin{align*}\n",
    "r = -(\\theta^2 + 0.1  \\cdot \\omega^2 + 0.001 \\cdot T_\\mathrm{u}^2).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcfda8",
   "metadata": {},
   "source": [
    "For further information about the environment kindly refer to the code and documentation of Farama Foundation's `gymnasium`:\n",
    "\n",
    "[Documentation of the gymnasium pendulum](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n",
    "\n",
    "[Pendulum environment in the gymnasium Github repository](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/pendulum.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efde99c",
   "metadata": {},
   "source": [
    "# Choose and implement a RL algorithm that is suitable for the given environment.\n",
    "<font color=\"red\">Note: Unlike in exercise 06, your agent should learn from interaction with the environment without discretization, i.e., must be naturally capable of handling continuous states and actions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e07ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb3048",
   "metadata": {},
   "source": [
    "Plot the learning curve of the training process. Clearly label axes (e.g., episode vs. total reward) and make it readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60273d43",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Test the learned policy considering a statistical significant number of episodes and report on the performance using representative metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd97158",
   "metadata": {},
   "source": [
    "# Save the learned policy to hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1244b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d333e",
   "metadata": {},
   "source": [
    "# Load the saved policy\n",
    "Make sure that the policy can be loaded and evaluated \n",
    "successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b608a",
   "metadata": {},
   "source": [
    "## What you need to submit\n",
    "\n",
    "Your final submission must include the following **components**:\n",
    "\n",
    "1. **Saved Agent**  \n",
    "   - Save your trained agent's policy/model weights.  \n",
    "   - Ensure the saved model can be loaded and evaluated again.\n",
    "\n",
    "2. **Jupyter Notebook (`.ipynb`)**  \n",
    "   - Submit a clean and well-documented notebook that contains:  \n",
    "     - The code for training the agent  \n",
    "     - Explanations of the implementation steps  \n",
    "     - Hyperparameters and design choices  \n",
    "     - Optional: visualizations or debugging outputs  \n",
    "   - Make sure the notebook can be rerun from top to bottom without errors.\n",
    "\n",
    "3. **Evaluation Video** (optional)  \n",
    "   - Record a short video (30–90 seconds) showing the trained agent performing the task.  \n",
    "   - You can use screen capture tools or any screen recorder of your choice. \n",
    "\n",
    "## ------------------------------------\n",
    "## Submission format\n",
    "\n",
    "Please submit a ZIP folder or a GitHub repository link that includes:\n",
    "\n",
    "```\n",
    "/YourProjectFolder\n",
    "│\n",
    "├── agent/                 # Saved agent file(s)\n",
    "├── notebook.ipynb         # Your Jupyter notebook\n",
    "├── evaluation_video.mp4   # (Optional) Short video of agent in action\n",
    "```\n",
    "## ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c38c2",
   "metadata": {},
   "source": [
    "## Exam session\n",
    "\n",
    "Each exam appointment will start with a **presentation and discussion session** on your individual submission. Be prepared to:\n",
    "\n",
    "- **Explain your algorithm choice**: Why did you choose this specific RL algorithm?\n",
    "- **Describe the implementation steps**: Walk through the key parts of your code.\n",
    "- **Discuss your results**: Show the learning curve, explain the agent's behavior (based on useful quantitative metrics), and reflect on challenges or improvements.\n",
    "\n",
    "A simple slide deck (e.g., based on LaTeX beamer class or similar) is recommended for clarity during your presentation.\n",
    "\n",
    "An exam appointment will take around 45 minutes in total. After the first part of the exam, your presentation and the discussion of your submission, a second part with general questions to the course content will follow. For the latter, it is expected that main reinforcement learning concepts including key assumptions and limitations can be explained. During the general exam talk, students are highly encouraged to highlight their explanations using key equations, diagram sketches as well as pseudocode summaries. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
